import os
import asyncio
import json
import logging
import re
import yaml
import sys
from typing import List, Dict, Any, Optional

# LangChain imports
import langchain
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.messages import SystemMessage, HumanMessage
from pydantic import BaseModel, Field

from app.core.config import settings
from app.services.github.github_client import GithubClient
from app.services.ai.ai_docs_site_generator import AiDocsBlogGenerator

# [Debugger Setup]
# ë¡œê·¸ ë ˆë²¨ ì„¤ì • (Celery í™˜ê²½ ëŒ€ì‘)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# -------------------------------------------------------------------------
# [Schemas]
# -------------------------------------------------------------------------
class RecommendedFile(BaseModel):
    path: str = Field(description="The exact file path")
    reason: str = Field(description="Reason for selection")
    score: int = Field(description="Relevance score (0-100)")

class RecommendationResult(BaseModel):
    files: List[RecommendedFile] = Field(description="List of selected files")

class DocsGeneratorService:
    def __init__(self, token: str):
        self.gh = GithubClient(token)
        self.ai_generator = AiDocsBlogGenerator()
        
        # ëª¨ë¸ ì„¤ì • (ë¹„ìš©/ì„±ëŠ¥ ê³ ë ¤: gpt-4o ì‚¬ìš© ê¶Œìž¥)
        self.scanner_llm = ChatOpenAI(
            model="gpt-4o",
            api_key=settings.OPENAI_API_KEY,
            temperature=0
        )

    # -------------------------------------------------------------------------
    # [NUCLEAR LOGGING] ê°•ì œ ì¶œë ¥ í•¨ìˆ˜ (Celery ë¡œê±° ë¬´ì‹œí•˜ê³  stdoutì— ì¨)
    # -------------------------------------------------------------------------
    def _force_print(self, title: str, content: str):
        try:
            separator = "=" * 60
            # í„°ë¯¸ë„ìš© í¬ë§· (Celery ë¡œê·¸ì—ì„œ ì‹ë³„í•˜ê¸° ì‰½ë„ë¡)
            log_message = f"\n{separator}\n[DEBUG: {title}]\n{separator}\n{content}\n{separator}\n"
            sys.__stdout__.write(log_message)
            sys.__stdout__.flush()
        except Exception:
            pass # ë¡œê¹… ì‹¤íŒ¨ë¡œ ë¡œì§ì´ ë©ˆì¶”ì§€ ì•Šë„ë¡ í•¨

    # -------------------------------------------------------------------------
    # Helper Methods
    # -------------------------------------------------------------------------
    def _parse_front_matter(self, content: str) -> (dict, str):
        """
        ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì—ì„œ YAML Front Matterì™€ ë³¸ë¬¸ì„ ë¶„ë¦¬í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        front_matter = {}
        body = content or "" # None ë°©ì–´
        
        pattern = r"^---\s+(.*?)\s+---\s+(.*)$"
        match = re.search(pattern, body, re.DOTALL)
        
        if match:
            yaml_text = match.group(1)
            body_text = match.group(2)
            try:
                front_matter = yaml.safe_load(yaml_text)
                return front_matter, body_text
            except Exception as e:
                logger.warning(f"YAML parsing failed: {e}")
                
        return front_matter, body

    def _is_ignored(self, path: str) -> bool:
        ignored_ext = {
            '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.pdf', '.zip', '.tar', '.gz', 
            '.woff', '.ttf', '.mp4', '.lock', '.txt', '.map', '.min.js', '.css', '.scss',
            '.json', '.xml', '.yml', '.yaml'
        }
        ignored_dirs = {
            'node_modules/', 'dist/', 'build/', '.git/', '__pycache__/', 'venv/', '_site/', 'docs/', 
            '.github/', 'public/', 'assets/'
        }
        if any(path.endswith(ext) for ext in ignored_ext): return True
        if any(d in path for d in ignored_dirs): return True
        return False

    # -------------------------------------------------------------------------
    # [Logic 1] AI File Scanning
    # -------------------------------------------------------------------------
    async def recommend_related_files(
        self, 
        repo_name: str, 
        branch: str, 
        doc_title: str, 
        doc_context: str = "", 
        doc_path: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        
        self._force_print("STEP 1: SCANNER START", f"Repo: {repo_name}\nGoal: {doc_title}")
        
        # 1. ë°ì´í„° ìˆ˜ì§‘
        all_files = await self.gh.fetch_all_file_paths(repo_name, branch)
        
        # [Fix] ë¸Œëžœì¹˜ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° (íŒŒì¼ 0ê°œ), ì‹¤ì œ ê¸°ë³¸ ë¸Œëžœì¹˜ë¡œ ìž¬ì‹œë„
        if not all_files:
            real_default = await self.gh.get_default_branch(repo_name)
            if real_default and real_default != branch:
                self._force_print("BRANCH CORRECTION", f"'{branch}' failed. Retrying with default: '{real_default}'")
                branch = real_default
                all_files = await self.gh.fetch_all_file_paths(repo_name, branch)

        filtered = [f for f in all_files if not self._is_ignored(f)]
        
        # íŒŒì¼ íŠ¸ë¦¬ ìƒì„± (Max 3000ê°œ)
        file_tree = "\n".join(filtered[:3000])
        
        try:
            readme = await self.gh.analyze_readme(repo_name, branch)
            tech = await self.gh.analyze_tech_stack(repo_name, branch)
        except:
            readme, tech = "N/A", "N/A"

        self._force_print("SCANNER INPUTS", f"Files: {len(filtered)}\nTech: {tech}\nContext: {doc_context}")

        # 2. LLM í˜¸ì¶œ
        ai_picks = await self._invoke_llm_scanner(
            file_list_str=file_tree,
            readme_summary=str(readme)[:1500],
            tech_stack=str(tech),
            doc_title=doc_title,
            user_context=doc_context
        )
        
        # 3. ê²°ê³¼ ë§¤í•‘
        result = []
        for path in filtered:
            item = {
                "path": path, 
                "name": os.path.basename(path), 
                "type": "file"
            }
            
            if path in ai_picks:
                d = ai_picks[path]
                item.update({
                    "recommended": True, 
                    "score": d['score'], 
                    "reason": d['reason']
                })
            else:
                item.update({
                    "recommended": False, 
                    "score": 0, 
                    "reason": None
                })
            result.append(item)
            
        # ì •ë ¬: ì¶”ì²œëœ ê²ƒì„ ìƒë‹¨ì— ë°°ì¹˜ (íŠ¸ë¦¬ êµ¬ì„± ì‹œì—ëŠ” ë¬´ì‹œë˜ê² ì§€ë§Œ ë¦¬ìŠ¤íŠ¸ ë·°ì—ì„œëŠ” ìœ ìš©)
        result.sort(key=lambda x: x['score'], reverse=True)
        
        self._force_print("SCANNER RESULT", f"Selected: {len(result)} files")
        return result

    async def _invoke_llm_scanner(self, file_list_str, readme_summary, tech_stack, doc_title, user_context):
        # f-string ë‚´ë¶€ì— JSON ìŠ¤í‚¤ë§ˆ({})ê°€ ë“¤ì–´ê°€ë©´ ì¶©ëŒë‚˜ë¯€ë¡œ ë³€ìˆ˜ ì£¼ìž… ë°©ì‹ ì‚¬ìš©
        parser = JsonOutputParser(pydantic_object=RecommendationResult)
        format_instructions = parser.get_format_instructions()
        
        system_content = "You are a Senior Software Architect. Select 3-7 files relevant to the documentation topic."
        
        human_content = f"""
        [Project Info]
        Stack: {tech_stack}
        Readme: {readme_summary}

        [Goal]
        Title: {doc_title}
        Context: {user_context}

        [Files]
        {file_list_str}

        [Task]
        Return the relevant files in JSON format.
        {format_instructions}
        """
        
        messages = [
            SystemMessage(content=system_content),
            HumanMessage(content=human_content)
        ]
        
        response = await self.scanner_llm.ainvoke(messages)
        
        try:
            parsed_result = parser.parse(response.content)
            return {item['path']: {'score': item['score'], 'reason': item['reason']} for item in parsed_result.get('files', [])}
        except Exception as e:
            logger.error(f"Scanner Parse Error: {e}")
            return {}


    # =================================================================
    # [Logic 2] Content Generation (Fixed: Tables & Mermaid 9.1.3)
    # =================================================================
    async def generate_content(
        self, 
        repo_name: str, 
        branch: str, 
        doc_path: Optional[str], 
        reference_files: List[str], 
        user_prompt: Optional[str] = "",
        doc_title: Optional[str] = None,
        doc_context: Optional[str] = None
    ) -> str:
        
        # [FIX: NoneType Error ë°©ì–´]
        safe_doc_path = doc_path
        if not safe_doc_path:
            safe_doc_path = "new_document.md"
            self._force_print("WARNING", "doc_path was None/Empty. Defaulting to 'new_document.md'")

        self._force_print("STEP 2: GENERATOR START", f"Target: {safe_doc_path}")
        
        # 1. Fetch Data (Target + References)
        target_file_task = self.gh.fetch_raw_content(repo_name, safe_doc_path, branch)
        
        # [Fix Preview] ë¨¼ì € contentë¥¼ í™•ì¸í•˜ì—¬ ë¸Œëžœì¹˜ê°€ ìœ íš¨í•œì§€ ì²´í¬ (Optimistic check)
        # ë§Œì•½ fetch ê²°ê³¼ê°€ 404/Emptyë¼ë©´ ë¸Œëžœì¹˜ë¥¼ ë°”ê¿”ì„œ ë‹¤ì‹œ ì‹œë„í•´ì•¼ í•¨.
        # í•˜ì§€ë§Œ gatherë¡œ ë³‘ë ¬ ìš”ì²­ ì¤‘ì´ë¼, ì—¬ê¸°ì„œ ì‚¬ì „ ì²´í¬ë¥¼ í•˜ë‚˜ë§Œ ë¨¼ì € ìˆ˜í–‰í•˜ê±°ë‚˜,
        # ê·¸ëƒ¥ ì•ˆì „í•˜ê²Œ get_default_branchë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì—¬ branchë¥¼ ê°±ì‹ í•˜ëŠ” ê²ƒì´ ë‚˜ìŒ (ì†ë„ vs ì •í™•ì„±).
        # ì—¬ê¸°ì„œëŠ” "ì‹¤íŒ¨ ì‹œ ìž¬ì‹œë„" ì „ëžµ ëŒ€ì‹ , "docs_generator"ëŠ” ì´ë¯¸ ê²€ì¦ëœ branchë¥¼ ë°›ëŠ”ë‹¤ê³  ê°€ì •í•˜ë˜,
        # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ README ì¡°íšŒë¡œ ë¸Œëžœì¹˜ ìƒì‚¬ í™•ì¸ì„ ë¨¼ì € í•¨.
        
        # (ê°„ì†Œí™”) READMEë¥¼ ë¨¼ì € ì°”ëŸ¬ë³´ê³  ì‹¤íŒ¨í•˜ë©´ ë¸Œëžœì¹˜ ë³€ê²½
        readme_check = await self.gh.analyze_readme(repo_name, branch)
        if readme_check == "No README found.":
            real_default = await self.gh.get_default_branch(repo_name)
            if real_default != branch:
                self._force_print("BRANCH CORRECTION", f"Retrying content gen with: {real_default}")
                branch = real_default

        # ë‹¤ì‹œ Task êµ¬ì„± (Updated Branch)
        target_file_task = self.gh.fetch_raw_content(repo_name, safe_doc_path, branch)
        unique_refs = [f for f in reference_files if f != safe_doc_path]
        ref_tasks = [self.gh.fetch_raw_content(repo_name, f, branch) for f in unique_refs]
        
        readme_task = self.gh.analyze_readme(repo_name, branch)
        tech_task = self.gh.analyze_tech_stack(repo_name, branch)
        
        results = await asyncio.gather(readme_task, tech_task, target_file_task, *ref_tasks)
        
        readme_content = results[0]
        tech_stack = results[1]
        raw_target_content = results[2] or "" # None ë°©ì§€
        ref_contents = results[3:]
        
        # 2. Parse & Context Processing
        fm_data, body_content = self._parse_front_matter(raw_target_content)
        
        final_title = doc_title or fm_data.get('title')
        if not final_title:
            base = os.path.basename(safe_doc_path).rsplit('.', 1)[0]
            final_title = base.replace("-", " ").replace("_", " ").title()
            
        user_instruction = user_prompt if user_prompt else doc_context
        if not user_instruction:
            user_instruction = "Analyze this code deeply and create a structured Wiki documentation."

        # 3. [PROMPT ENGINEERING]
        
        # (1) íŒŒì¼ ìœ í˜•ë³„ ë¶„ì„ ê°€ì´ë“œ
        analysis_guideline = """
        [ANALYSIS STRATEGY]
        Analyze the [REFERENCE SOURCES] based on their file types:
        1. **Router/Urls**: Create a Markdown TABLE of `[Route Path]`, `[HTTP Method]`, `[Handler Function]`, `[Purpose]`.
        2. **API/Views/Controllers**: Create a TABLE of `[Endpoint]`, `[Input Params]`, `[Response Model]`, `[Logic Summary]`.
        3. **Models/Schemas**: Create a TABLE of `[Field Name]`, `[Type]`, `[Constraints]`, `[Description]`.
        4. **Business Logic/Services**: Analyze the core logic flow and describe it in a structured list or text steps.
        """

        # (2) Just-the-Docs ìŠ¤íƒ€ì¼ ê°€ì´ë“œ (ìƒì„¸ ë²„ì „ + Mermaid Fix + Table Fix)
        style_guide = """
        [STYLE RULES - Just the Docs]
        1. **Language**: **KOREAN (í•œêµ­ì–´)**. Use a professional, technical tone (e.g., "ì œê³µí•©ë‹ˆë‹¤", "êµ¬í˜„ë˜ì–´ ìžˆìŠµë‹ˆë‹¤").
        2. **Badges**: Use labels for status/type. Syntax: `{: .label .label-blue }`
        3. **No Mermaid**: Do NOT generate Mermaid diagrams. Use text descriptions instead.
        4. **Tables (CRITICAL - STRICT COMPLIANCE REQUIRED)**:
           - **Rule 1**: You MUST insert an **Empty Line** before and after the table.
           - **Rule 2**: You MUST include the **Alignment Row** (second row) with hyphens and colons (e.g., `|:---|---:|`).
           - **Rule 3**: Do NOT indent the table. Keep it at the start of the line.
           - **Example (CORRECT)**:
             
             | Name | Type | Description |
             |:-----|:-----|:------------|
             | id   | int  | Primary Key |
             
        5. **No Front Matter**: Do NOT include YAML front matter (--- ... ---) at the beginning. Start with the Title (# Title).
        6. **No Chat**: Do NOT output conversational fillers like "Here is the document" or "How can I help next?". Output ONLY the documentation content.
        """

        # (3) ë¬¸ì„œ êµ¬ì¡° í…œí”Œë¦¿
        structure_template = f"""
        [Output Structure]
        ---
        title: {final_title}
        description: Generated by Eggit AI
        layout: default
        ---
        
        # {final_title}
        
        ## 1. ê°œìš” (Overview)
        (Summarize the role of this module. Use badges for status or tech.)
        

[Image of software architecture diagram]


        ## 2. ì•„í‚¤í…ì²˜ ë° ë¡œì§ (Architecture & Logic)
        (Explain the internal flow and relationships between components systematically using the reference code.)
        
        ## 3. í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ë¶„ì„ (Key Components)
        (Detailed analysis. **Use Markdown TABLES here**.)
        
        ### 3.1 ì£¼ìš” í•¨ìˆ˜/í´ëž˜ìŠ¤
        - **ì„¤ëª…**: ...
        - **ìƒì„¸ ëª…ì„¸ (í…Œì´ë¸”)**:
          | íŒŒë¼ë¯¸í„° | íƒ€ìž… | ì„¤ëª… |
          |:--------|:-----|:-----|
          | ...     | ...  | ...  |
        
        ## 4. ì‚¬ìš© ì˜ˆì‹œ (Usage)
        (Code snippets or API examples.)
        
        ## 5. ì„¤ì • (Configuration)
        - **í™˜ê²½ ë³€ìˆ˜**: (Markdown Table with `|:---|` alignment)
        """

        # (4) ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
        system_prompt = f"""
        You are a **Principal Software Architect** and **Technical Writer**.
        
        **YOUR MISSION**:
        1. **Strict Data-Driven Documentation**: Use the **[REFERENCE SOURCES]** (Project Data Code) as the absolute source of truth.
        2. **Deep Analysis**: Analyze the code structure, logic, and data flow deeply from the provided reference files.
        3. **Rich Output**: Create a detailed document with **Tables** (must have alignment row).
        4. **Language**: Output strictly in **KOREAN (í•œêµ­ì–´)**.
        5. **No Chatbot Persona**: Do not output any conversational text. Just the document.
        
        {analysis_guideline}
        {style_guide}
        {structure_template}
        """
        
        # (5) ìœ ì € ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½
        draft_hint = doc_context if doc_context else raw_target_content
        current_draft_context = f"""
        ### ðŸ“Œ [CURRENT DRAFT INFO] (Title: {final_title})
        (Use this only to understand the topic. Do NOT just repeat this.)
        ```markdown
        {draft_hint[:3000]} 
        ```
        """

        ref_context_str = ""
        for path, code in zip(unique_refs, ref_contents):
            safe_code = code or ""
            # ì†ŒìŠ¤ ì½”ë“œëŠ” ë¶„ì„ì˜ í•µì‹¬ì´ë¯€ë¡œ ìµœëŒ€í•œ ë§Žì´(30000ìž) ìž…ë ¥
            snippet = safe_code[:30000] + ("\n...(truncated)" if len(safe_code) > 30000 else "")
            ref_context_str += f"\n### ðŸ”— [REFERENCE SOURCE] {path}\n```\n{snippet}\n```\n"

        user_input_context = f"""
        [Task Metadata]
        - **Target File**: {safe_doc_path}
        - **User Request**: "{user_instruction}"
        
        [Project Context]
        - **Tech Stack**: {tech_stack}
        
        {current_draft_context}
        
        [REFERENCE SOURCES - ANALYZE DEEPLY]
        {ref_context_str}
        
        [Final Instruction]
        Synthesize the **[REFERENCE SOURCES]** into a structured Wiki page titled **"{final_title}"**.
        **Strictly follow the Table and Mermaid syntax rules.**
        **Output in KOREAN.**
        """
        
        # ------------------------------------------------------------------
        # [NUCLEAR DEBUGGING]
        # ------------------------------------------------------------------
        
        self._force_print("GENERATOR INPUTS", f"Ref Files: {len(unique_refs)}")

        try:
            with open("last_ai_prompt_debug.txt", "w", encoding="utf-8") as f:
                f.write(f"=== SYSTEM PROMPT ===\n{system_prompt}\n\n{'='*50}\n\n=== USER CONTEXT ===\n{user_input_context}")
            self._force_print("DEBUG FILE", f"Saved full prompt to: {os.path.abspath('last_ai_prompt_debug.txt')}")
        except Exception as e:
            self._force_print("DEBUG FILE ERROR", str(e))

        logger.info("[DocsAI] ðŸ§  Sending request to Writer LLM...")
        result_md = await self.ai_generator.generate_content(system_prompt, user_input_context)
        
        self._force_print("COMPLETE", f"Generated Length: {len(result_md)}")
        return result_md